{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/2_tensor_program_abstraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing FP32 vs Int8 latency\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GEqpO14Lf0Lq"
   },
   "source": [
    "Use the Matrix multiplication program from the Machine Learning Compiler (MLC) course to explore the FP32 vs Int8 optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9ExHE3BfYYv",
    "outputId": "0d82d527-8051-4bc3-c3a7-0cbb12d9bcce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: float32    Baseline latency:  1.507176\n",
      "Splitting tensors into    8 word blocks: 0.305563\n",
      "Splitting tensors into   16 word blocks: 0.237635\n",
      "Splitting tensors into   32 word blocks: 0.199116\n",
      "Splitting tensors into   64 word blocks: 0.167480\n",
      "Splitting tensors into  128 word blocks: 0.128696\n",
      "Splitting tensors into  256 word blocks: 0.143246\n",
      "Splitting tensors into  512 word blocks: 0.140489\n",
      "Splitting tensors into 1024 word blocks: 0.139810\n",
      "Splitting tensors into 2048 word blocks: 1.309801\n",
      "\n",
      "\n",
      "Data Type:    int8    Baseline latency:  1.686101\n",
      "Splitting tensors into    8 word blocks: 0.139921\n",
      "Splitting tensors into   16 word blocks: 0.092403\n",
      "Splitting tensors into   32 word blocks: 0.042042\n",
      "Splitting tensors into   64 word blocks: 0.036159\n",
      "Splitting tensors into  128 word blocks: 0.032969\n",
      "Splitting tensors into  256 word blocks: 0.042193\n",
      "Splitting tensors into  512 word blocks: 0.046457\n",
      "Splitting tensors into 1024 word blocks: 0.036784\n",
      "Splitting tensors into 2048 word blocks: 2.693391\n",
      "\n",
      "\n",
      "Reordered schedule:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
       "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
       "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
       "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(\n",
       "        A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;int8&quot;</span>),\n",
       "        B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;int8&quot;</span>),\n",
       "        C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">1024</span>), <span style=\"color: #BA2121\">&quot;int8&quot;</span>),\n",
       "    ):\n",
       "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: <span style=\"color: #008000; font-weight: bold\">True</span>})\n",
       "        <span style=\"color: #008000; font-weight: bold\">for</span> m_0, n_0, k, m_1, n_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">1024</span>, <span style=\"color: #008000\">2048</span>, <span style=\"color: #008000\">2048</span>):\n",
       "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
       "                v_m <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, m_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2048</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> m_1)\n",
       "                v_n <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, n_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2048</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_1)\n",
       "                v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">1024</span>, k)\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>where(m_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2048</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> m_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">1024</span> <span style=\"color: #008000; font-weight: bold\">and</span> n_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2048</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> n_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">1024</span>)\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_m, v_k], B[v_k, v_n])\n",
       "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[v_m, v_n])\n",
       "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
       "                    C[v_m, v_n] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int8(<span style=\"color: #008000\">0</span>)\n",
       "                C[v_m, v_n] <span style=\"color: #AA22FF; font-weight: bold\">=</span> C[v_m, v_n] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[v_m, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[v_k, v_n]\n",
       "</pre></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm import te\n",
    "import numpy as np\n",
    "\n",
    "# Dimensions of the matrices\n",
    "M = 1024\n",
    "K = 1024\n",
    "N = 1024\n",
    "\n",
    "# Update the flag below for the desired HW target\n",
    "target = \"llvm -mcpu=skylake\"\n",
    "dev = tvm.device(target, 0)\n",
    "\n",
    "# Iterate over the data types\n",
    "for dtype in [\"float32\", \"int8\"]:\n",
    "    # Algorithm\n",
    "    k = te.reduce_axis((0, K), \"k\")\n",
    "    A = te.placeholder((M, K), dtype, name=\"A\")\n",
    "    B = te.placeholder((K, N), dtype, name=\"B\")\n",
    "    C = te.compute((M, N), lambda m, n: te.sum(A[m, k] * B[k, n], axis=k), name=\"C\")\n",
    "\n",
    "    # Default schedule\n",
    "    func = te.create_prim_func([A, B, C])\n",
    "    func = func.with_attr(\"global_symbol\", \"main\")\n",
    "    ir_module = IRModule({\"main\": func})\n",
    "\n",
    "    func = tvm.build(ir_module, target)  # The module for CPU backends.\n",
    "\n",
    "    a = tvm.nd.array(np.random.rand(M, K).astype(dtype), dev)\n",
    "    b = tvm.nd.array(np.random.rand(K, N).astype(dtype), dev)\n",
    "    c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
    "    func(a, b, c)\n",
    "\n",
    "    evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "    print(\"Data Type:\", dtype.rjust(7, ' '), \"   Baseline latency:  %f\" % evaluator(a, b, c).mean)\n",
    "\n",
    "    for block_size in [8, 16, 32, 64, 128, 256, 512, 1024, 2048]:\n",
    "        # Now change the schedule taking into consideration the cache\n",
    "        sch = tvm.tir.Schedule(ir_module)\n",
    "        block_c = sch.get_block(\"C\")\n",
    "        # Get loops surronding the block\n",
    "        (y, x, k) = sch.get_loops(block_c)\n",
    "        yo, yi = sch.split(y, [None, block_size])\n",
    "        xo, xi = sch.split(x, [None, block_size])\n",
    "\n",
    "        sch.reorder(yo, xo, k, yi, xi)\n",
    "\n",
    "        func = tvm.build(sch.mod, target=\"llvm -mcpu=skylake\")  # The module for CPU backends.\n",
    "\n",
    "        c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
    "        func(a, b, c)\n",
    "\n",
    "        evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "        print(\"Splitting tensors into\", str(block_size).rjust(4, ' '),\n",
    "              \"word blocks: %f\" % evaluator(a, b, c).mean)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Reordered schedule:\")\n",
    "sch.mod.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2-tensor-program-abstraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f9f1961f3cd2aa30cee50c488a9e053b355d17c1c2c8f287ecc8b2b5b31144d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
