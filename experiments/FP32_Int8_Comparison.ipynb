{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mlc-ai/notebooks/blob/main/2_tensor_program_abstraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing FP32 vs Int8 latency\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GEqpO14Lf0Lq"
   },
   "source": [
    "Use the Matrix multiplication program from the Machine Learning Compiler (MLC) course to explore the FP32 vs Int8 optimizations. The goal is to explore how the default schedule can be improved by splitting the loops of the comptute block.\n",
    "\n",
    "Start with breaking up the outer loops in the compute block to increase temporal locality\n",
    "\n",
    "### Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9ExHE3BfYYv",
    "outputId": "0d82d527-8051-4bc3-c3a7-0cbb12d9bcce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: float32    Baseline latency:  1.508090\n",
      "Splitting tensors into    8 word blocks: 0.3096\n",
      "Splitting tensors into   16 word blocks: 0.2408\n",
      "Splitting tensors into   32 word blocks: 0.1989\n",
      "Splitting tensors into   64 word blocks: 0.1714\n",
      "Splitting tensors into  128 word blocks: 0.1301\n",
      "Splitting tensors into  256 word blocks: 0.1541\n",
      "Splitting tensors into  512 word blocks: 0.1462\n",
      "Splitting tensors into 1024 word blocks: 0.1367\n",
      "\n",
      "Data Type:    int8    Baseline latency:  1.672048\n",
      "Splitting tensors into    8 word blocks: 0.1434\n",
      "Splitting tensors into   16 word blocks: 0.0943\n",
      "Splitting tensors into   32 word blocks: 0.0409\n",
      "Splitting tensors into   64 word blocks: 0.0363\n",
      "Splitting tensors into  128 word blocks: 0.0335\n",
      "Splitting tensors into  256 word blocks: 0.0414\n",
      "Splitting tensors into  512 word blocks: 0.0431\n",
      "Splitting tensors into 1024 word blocks: 0.0360\n",
      "\n",
      "Best FP32 is block_size  128 with latency 0.1301\n",
      "Best Int8 is block_size  128 with latency 0.0335\n",
      "Int8 speedup over FP32 is 287.959840%\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm import te\n",
    "import numpy as np\n",
    "\n",
    "# Dimensions of the matrices\n",
    "M = 1024\n",
    "K = 1024\n",
    "N = 1024\n",
    "\n",
    "# Update the flag below for the desired HW target\n",
    "target = \"llvm -mcpu=skylake\"\n",
    "dev = tvm.device(target, 0)\n",
    "\n",
    "# Iterate over the data types, first create the result dictionaries\n",
    "result_fp_dict = dict()\n",
    "result_int_dict = dict()\n",
    "\n",
    "for dtype in [\"float32\", \"int8\"]:\n",
    "    # Algorithm\n",
    "    k = te.reduce_axis((0, K), \"k\")\n",
    "    A = te.placeholder((M, K), dtype, name=\"A\")\n",
    "    B = te.placeholder((K, N), dtype, name=\"B\")\n",
    "    C = te.compute((M, N), lambda m, n: te.sum(A[m, k] * B[k, n], axis=k), name=\"C\")\n",
    "\n",
    "    # Default schedule\n",
    "    func = te.create_prim_func([A, B, C])\n",
    "    func = func.with_attr(\"global_symbol\", \"main\")\n",
    "    ir_module = IRModule({\"main\": func})\n",
    "    \n",
    "    func = tvm.build(ir_module, target)  # The module for CPU backends.\n",
    "\n",
    "    a = tvm.nd.array(np.random.rand(M, K).astype(dtype), dev)\n",
    "    b = tvm.nd.array(np.random.rand(K, N).astype(dtype), dev)\n",
    "    c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
    "    func(a, b, c)\n",
    "\n",
    "    evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "    print(\"Data Type:\", dtype.rjust(7, ' '), \"   Baseline latency:  %f\" % evaluator(a, b, c).mean)\n",
    "\n",
    "    for block_size in [8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "        # Now change the schedule taking into consideration the cache\n",
    "        sch = tvm.tir.Schedule(ir_module)\n",
    "        block_c = sch.get_block(\"C\")\n",
    "        # Break the outer loops based on the cache block_size\n",
    "        (y, x, k) = sch.get_loops(block_c)\n",
    "        yo, yi = sch.split(y, [None, block_size])\n",
    "        xo, xi = sch.split(x, [None, block_size])\n",
    "\n",
    "        sch.reorder(yo, xo, k, yi, xi)\n",
    "\n",
    "        func = tvm.build(sch.mod, target=\"llvm -mcpu=skylake\")  # The module for CPU backends.\n",
    "\n",
    "        c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
    "        func(a, b, c)\n",
    "\n",
    "        evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "        result = evaluator(a, b, c).mean\n",
    "        print(\"Splitting tensors into\", str(block_size).rjust(4, ' '),\n",
    "              \"word blocks: %.4f\" % result)\n",
    "        \n",
    "             # Create a result dictionary for both FP and int8\n",
    "        if dtype == \"float32\":\n",
    "            result_fp_dict.update({block_size:result})\n",
    "        elif dtype == \"int8\":\n",
    "            result_int_dict.update({block_size:result})\n",
    "        else:\n",
    "            print(\"Something went wrong\")\n",
    "            pass\n",
    "    \n",
    "    print(\"\")\n",
    "          \n",
    "best = sorted(result_fp_dict.items(), key=lambda x:x[1])[0]\n",
    "fp32=best[1]\n",
    "print(\"Best FP32 is block_size\", str(best[0]).rjust(4, ' '),\n",
    "      \"with latency %.4f\" % fp32)\n",
    "\n",
    "best = sorted(result_int_dict.items(), key=lambda x:x[1])[0]\n",
    "int8=best[1]\n",
    "print(\"Best Int8 is block_size\", str(best[0]).rjust(4, ' '),\n",
    "      \"with latency %.4f\" % int8)\n",
    "print(\"Int8 speedup over FP32 is %f%%\" % ((np.divide(fp32, int8) - 1.0) * 100.0))\n",
    "block_size = int(best[0]) # Used in the block below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the best block_size optimizations look like in the TIR code. Note that the optimization includes any arithmetic speedup of int8 vs FP32. This experiment does not separate memory and compute performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Int8 schedule\")\n",
    "ir_module.show()\n",
    "\n",
    "print(\"Reordered schedule:\")\n",
    "sch = tvm.tir.Schedule(ir_module)\n",
    "block_c = sch.get_block(\"C\")\n",
    "# Get loops surronding the block\n",
    "(y, x, k) = sch.get_loops(block_c)\n",
    "yo, yi = sch.split(y, [None, block_size])\n",
    "xo, xi = sch.split(x, [None, block_size])\n",
    "sch.reorder(yo, xo, k, yi, xi)\n",
    "\n",
    "sch.mod.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the middle loop only\n",
    "Lets explorewhat happens when we only split the middle loop. This may improve cache locality compared to the above scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: float32    Baseline latency:  1.514097\n",
      "Splitting tensors into    8 word blocks: 0.6477\n",
      "Splitting tensors into   16 word blocks: 0.4232\n",
      "Splitting tensors into   32 word blocks: 0.1479\n",
      "Splitting tensors into   64 word blocks: 0.1386\n",
      "Splitting tensors into  128 word blocks: 0.1213\n",
      "Splitting tensors into  256 word blocks: 0.1083\n",
      "Splitting tensors into  512 word blocks: 0.0951\n",
      "Splitting tensors into 1024 word blocks: 0.0896\n",
      "\n",
      "Data Type:    int8    Baseline latency:  1.685904\n",
      "Splitting tensors into    8 word blocks: 0.2565\n",
      "Splitting tensors into   16 word blocks: 0.1558\n",
      "Splitting tensors into   32 word blocks: 0.0912\n",
      "Splitting tensors into   64 word blocks: 0.0799\n",
      "Splitting tensors into  128 word blocks: 0.0712\n",
      "Splitting tensors into  256 word blocks: 0.0561\n",
      "Splitting tensors into  512 word blocks: 0.0415\n",
      "Splitting tensors into 1024 word blocks: 0.0377\n",
      "\n",
      "Best FP32 is block_size 1024 with latency 0.0896\n",
      "Best Int8 is block_size 1024 with latency 0.0377\n",
      "Int8 speedup over FP32 is 137.418788%\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm import te\n",
    "import numpy as np\n",
    "\n",
    "# Dimensions of the matrices\n",
    "M = 1024\n",
    "K = 1024\n",
    "N = 1024\n",
    "\n",
    "# Update the flag below for the desired HW target\n",
    "target = \"llvm -mcpu=skylake\"\n",
    "dev = tvm.device(target, 0)\n",
    "\n",
    "# Iterate over the data types, first create the result dictionaries\n",
    "result_fp_dict = dict()\n",
    "result_int_dict = dict()\n",
    "\n",
    "for dtype in [\"float32\", \"int8\"]:\n",
    "    # Algorithm\n",
    "    k = te.reduce_axis((0, K), \"k\")\n",
    "    A = te.placeholder((M, K), dtype, name=\"A\")\n",
    "    B = te.placeholder((K, N), dtype, name=\"B\")\n",
    "    C = te.compute((M, N), lambda m, n: te.sum(A[m, k] * B[k, n], axis=k), name=\"C\")\n",
    "\n",
    "    # Default schedule\n",
    "    func = te.create_prim_func([A, B, C])\n",
    "    func = func.with_attr(\"global_symbol\", \"main\")\n",
    "    ir_module = IRModule({\"main\": func})\n",
    "    \n",
    "    func = tvm.build(ir_module, target)  # The module for CPU backends.\n",
    "\n",
    "    a = tvm.nd.array(np.random.rand(M, K).astype(dtype), dev)\n",
    "    b = tvm.nd.array(np.random.rand(K, N).astype(dtype), dev)\n",
    "    c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
    "    func(a, b, c)\n",
    "\n",
    "    evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "    print(\"Data Type:\", dtype.rjust(7, ' '), \"   Baseline latency:  %f\" % evaluator(a, b, c).mean)\n",
    "\n",
    "    for block_size in [8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "        # Now change the schedule taking into consideration the cache\n",
    "        sch = tvm.tir.Schedule(ir_module)\n",
    "        block_c = sch.get_block(\"C\")\n",
    "        # Break the outer loops based on the cache block_size\n",
    "        (y, x, k) = sch.get_loops(block_c)\n",
    "        xo, xi = sch.split(x, [None, block_size])\n",
    "\n",
    "        sch.reorder(y, xo, k, xi)\n",
    "\n",
    "        func = tvm.build(sch.mod, target=\"llvm -mcpu=skylake\")  # The module for CPU backends.\n",
    "\n",
    "        c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
    "        func(a, b, c)\n",
    "\n",
    "        evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "        result = evaluator(a, b, c).mean\n",
    "        print(\"Splitting tensors into\", str(block_size).rjust(4, ' '),\n",
    "              \"word blocks: %.4f\" % result)\n",
    "        \n",
    "             # Create a result dictionary for both FP and int8\n",
    "        if dtype == \"float32\":\n",
    "            result_fp_dict.update({block_size:result})\n",
    "        elif dtype == \"int8\":\n",
    "            result_int_dict.update({block_size:result})\n",
    "        else:\n",
    "            print(\"Something went wrong\")\n",
    "            pass\n",
    "\n",
    "    print(\"\")\n",
    "    \n",
    "best = sorted(result_fp_dict.items(), key=lambda x:x[1])[0]\n",
    "fp32=best[1]\n",
    "print(\"Best FP32 is block_size\", str(best[0]).rjust(4, ' '),\n",
    "      \"with latency %.4f\" % fp32)\n",
    "\n",
    "best = sorted(result_int_dict.items(), key=lambda x:x[1])[0]\n",
    "int8=best[1]\n",
    "print(\"Best Int8 is block_size\", str(best[0]).rjust(4, ' '),\n",
    "      \"with latency %.4f\" % int8)\n",
    "print(\"Int8 speedup over FP32 is %f%%\" % ((np.divide(fp32, int8) - 1.0) * 100.0))\n",
    "block_size = int(best[0]) # Used in the block below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets explore the vectorization capabilities only. For this focus on the innermost loop\n",
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: float32    Baseline latency:  1.507966\n",
      "Splitting tensors into    8 word blocks: 1.4972\n",
      "Splitting tensors into   16 word blocks: 1.7023\n",
      "Splitting tensors into   32 word blocks: 1.6099\n",
      "Splitting tensors into   64 word blocks: 1.5219\n",
      "Splitting tensors into  128 word blocks: 1.5205\n",
      "Splitting tensors into  256 word blocks: 1.5367\n",
      "\n",
      "Data Type:    int8    Baseline latency:  1.687452\n",
      "Splitting tensors into    8 word blocks: 1.7013\n",
      "Splitting tensors into   16 word blocks: 1.7037\n",
      "Splitting tensors into   32 word blocks: 1.6747\n",
      "Splitting tensors into   64 word blocks: 1.6870\n",
      "Splitting tensors into  128 word blocks: 1.7167\n",
      "Splitting tensors into  256 word blocks: 1.7023\n",
      "\n",
      "Best FP32 is block_size    8 with latency 1.4972\n",
      "Best Int8 is block_size   32 with latency 1.6747\n",
      "Int8 speedup over FP32 is -10.596955%\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm import te\n",
    "import numpy as np\n",
    "\n",
    "# Dimensions of the matrices\n",
    "M = 1024\n",
    "K = 1024\n",
    "N = 1024\n",
    "\n",
    "# Update the flag below for the desired HW target\n",
    "target = \"llvm -mcpu=skylake\"\n",
    "dev = tvm.device(target, 0)\n",
    "\n",
    "# Iterate over the data types, first create the result dictionaries\n",
    "result_fp_dict = dict()\n",
    "result_int_dict = dict()\n",
    "\n",
    "for dtype in [\"float32\", \"int8\"]:\n",
    "    # Algorithm\n",
    "    k = te.reduce_axis((0, K), \"k\")\n",
    "    A = te.placeholder((M, K), dtype, name=\"A\")\n",
    "    B = te.placeholder((K, N), dtype, name=\"B\")\n",
    "    C = te.compute((M, N), lambda m, n: te.sum(A[m, k] * B[k, n], axis=k), name=\"C\")\n",
    "\n",
    "    # Default schedule\n",
    "    func = te.create_prim_func([A, B, C])\n",
    "    func = func.with_attr(\"global_symbol\", \"main\")\n",
    "    ir_module = IRModule({\"main\": func})\n",
    "    \n",
    "    func = tvm.build(ir_module, target)  # The module for CPU backends.\n",
    "\n",
    "    a = tvm.nd.array(np.random.rand(M, K).astype(dtype), dev)\n",
    "    b = tvm.nd.array(np.random.rand(K, N).astype(dtype), dev)\n",
    "    c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
    "    func(a, b, c)\n",
    "\n",
    "    evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "    print(\"Data Type:\", dtype.rjust(7, ' '), \"   Baseline latency:  %f\" % evaluator(a, b, c).mean)\n",
    "\n",
    "    for block_size in [8, 16, 32, 64, 128, 256]:\n",
    "        # Now change the schedule taking into consideration the cache\n",
    "        sch = tvm.tir.Schedule(ir_module)\n",
    "        block_c = sch.get_block(\"C\")\n",
    "        # Break the inner loop based on the SIMD vector unit length\n",
    "        (y, x, k) = sch.get_loops(block_c)\n",
    "        ko, ki = sch.split(k, [None, block_size])\n",
    "\n",
    "        sch.reorder(y, x, ko, ki)\n",
    "\n",
    "        func = tvm.build(sch.mod, target=\"llvm -mcpu=skylake\")  # The module for CPU backends.\n",
    "\n",
    "        c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
    "        func(a, b, c)\n",
    "\n",
    "        evaluator = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "        result = evaluator(a, b, c).mean\n",
    "        print(\"Splitting tensors into\", str(block_size).rjust(4, ' '),\n",
    "              \"word blocks: %.4f\" % result)\n",
    "        \n",
    "             # Create a result dictionary for both FP and int8\n",
    "        if dtype == \"float32\":\n",
    "            result_fp_dict.update({block_size:result})\n",
    "        elif dtype == \"int8\":\n",
    "            result_int_dict.update({block_size:result})\n",
    "        else:\n",
    "            print(\"Something went wrong\")\n",
    "            pass\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "best = sorted(result_fp_dict.items(), key=lambda x:x[1])[0]\n",
    "fp32=best[1]\n",
    "print(\"Best FP32 is block_size\", str(best[0]).rjust(4, ' '),\n",
    "      \"with latency %.4f\" % fp32)\n",
    "\n",
    "best = sorted(result_int_dict.items(), key=lambda x:x[1])[0]\n",
    "int8=best[1]\n",
    "print(\"Best Int8 is block_size\", str(best[0]).rjust(4, ' '),\n",
    "      \"with latency %.4f\" % int8)\n",
    "print(\"Int8 speedup over FP32 is %f%%\" % ((np.divide(fp32, int8) - 1.0) * 100.0))\n",
    "block_size = int(best[0]) # Used in the block below"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no improvement compred to FP32 and little improvement changing block sizes in the inner loop. Thus most likely any improvements through vectorization are negated by the poor cache temporal locality."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2-tensor-program-abstraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f9f1961f3cd2aa30cee50c488a9e053b355d17c1c2c8f287ecc8b2b5b31144d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
